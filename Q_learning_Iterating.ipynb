{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "#from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "import gym\n",
    "import myfrozen\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env,Q,learning_rate,gamma,episode,zerostates):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    t_reward = 0\n",
    "    max_steps = 100\n",
    "\n",
    "    move_counter = 0\n",
    "    for j in range(max_steps):\n",
    "        if done:\n",
    "            if j < max_steps:\n",
    "                zerostates.add(observation)\n",
    "            break\n",
    "\n",
    "        curr_state = observation\n",
    "\n",
    "        #action = np.argmax(Q[curr_state,:]  + np.random.randn(1, env.action_space.n)*(1./(episode+1)))\n",
    "        action = np.argmax(Q[curr_state,:]  + np.random.randn(1, env.action_space.n))\n",
    "        \n",
    "        move_counter+=1\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        collect.append((curr_state,action,observation,reward))\n",
    "\n",
    "        t_reward += reward\n",
    "\n",
    "        #Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "        Q[curr_state,action] += learning_rate * (reward+ gamma*np.max(Q[observation,:])-Q[curr_state,action])\n",
    "\n",
    "    return Q, t_reward, done, move_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(iteration,epochs=1000,learning_rate = 0.81,discount = 0.96):\n",
    "    \n",
    "    reward_per_ep = list()\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    \n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        #print i\n",
    "        Q, t_reward, done, move_counter = run_episode(env,Q,learning_rate,discount,i,zerostates)\n",
    "        #print done\n",
    "        reward_per_ep.append(t_reward)\n",
    "\n",
    "        if done:\n",
    "            if t_reward > 0 : # Win\n",
    "                wins += 1\n",
    "            else: # Loss\n",
    "                losses += 1\n",
    "        time.sleep(.1)        \n",
    "        clear_output(wait=True)\n",
    "        print(\"Board #: %s\" % (iteration+1,))\n",
    "        print(\"Game #: %s\" % (i+1,))\n",
    "        print(\"Moves this round %s\" % move_counter)\n",
    "        print(\"Final Position:\")\n",
    "        env.render()\n",
    "        print(\"Wins/Losses %s/%s\" % (wins, losses))\n",
    "\n",
    "    return Q, reward_per_ep, collect, zerostates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(collect):\n",
    "    df = pd.DataFrame(data=collect, columns=['state','action','state_new','reward'])\n",
    "\n",
    "    one_hot = pd.get_dummies(df['state'],prefix='state')\n",
    "    df = df.drop('state',axis=1)\n",
    "    df = df.join(one_hot)\n",
    "    one_hot = pd.get_dummies(df['state_new'],prefix='new_state')\n",
    "    df = df.drop('state_new',axis=1)\n",
    "    df = df.join(one_hot)\n",
    "    one_hot = pd.get_dummies(df['action'],prefix='action')\n",
    "    df = df.drop('action',axis=1)\n",
    "    df = df.join(one_hot)\n",
    "\n",
    "    target = pd.DataFrame(df.reward, columns=[\"reward\"])\n",
    "    df = df.drop('reward',axis=1)\n",
    "\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(df,target['reward'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(size,value):\n",
    "  my_onehot = np.zeros((size))\n",
    "  my_onehot[value] = 1.0\n",
    "  return my_onehot\n",
    "\n",
    "def make_environment(name,seed):\n",
    "    reload(gym.envs.registration)\n",
    "    register(\n",
    "            id=name,\n",
    "            entry_point='myfrozen.frozen_lake:FrozenLakeEnv',\n",
    "            kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    "            timestep_limit=100,\n",
    "            reward_threshold=0.78,\n",
    "            )\n",
    "    env = gym.make(name)\n",
    "    return env\n",
    "\n",
    "def fill_dict(D,MY_ENV_NAME,env,q,zerostates,policy,grid,model):\n",
    "    D[MY_ENV_NAME][\"env\"] = env\n",
    "    D[MY_ENV_NAME][\"q\"] = q\n",
    "    D[MY_ENV_NAME][\"zerostates\"] = zerostates\n",
    "    D[MY_ENV_NAME][\"policy\"] = policy\n",
    "    D[MY_ENV_NAME][\"grid\"] = grid \n",
    "    D[MY_ENV_NAME][\"model\"] = model\n",
    "    return D\n",
    "\n",
    "def make_policy(OBSERVATION_SPACE,q):\n",
    "    policy = np.zeros(OBSERVATION_SPACE)\n",
    "    for i in range(OBSERVATION_SPACE):\n",
    "        policy[i] = np.argmax(q[i,:])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_policy(initial_state):\n",
    "    A2A=['<','v','>','^']\n",
    "    grid = np.zeros((OBS_SQR,OBS_SQR), dtype='<U2')\n",
    "    for x in range(0,OBS_SQR):\n",
    "        for y in range(0,OBS_SQR):\n",
    "            my_state = initial_state.copy()\n",
    "            my_state[x,y] = 1\n",
    "\n",
    "            obs_predict = my_state.reshape(1,OBSERVATION_SPACE,)\n",
    "            obs_predict = np.squeeze(obs_predict)\n",
    "            index, = np.where(obs_predict == 1.)\n",
    "            action = np.argmax(q[index,:])\n",
    "            grid[x,y] = A2A[action]\n",
    "    grid\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Board #: 1\n",
      "Game #: 1000\n",
      "Moves this round 76\n",
      "Final Position:\n",
      "  (Left)\n",
      "FFFFFFSF\n",
      "FFFFFFFH\n",
      "FFFHFFFF\n",
      "FHFFFFFF\n",
      "HFFFFFFF\n",
      "FFFFFFHF\n",
      "FHFFFFFF\n",
      "FFFFFF\u001b[41mG\u001b[0mF\n",
      "Wins/Losses 93/907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/bgan/lib/python2.7/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "D = {}\n",
    "n_iters = 1\n",
    "for iteration in xrange(n_iters):\n",
    "    name='FrozenLakeNonskid8x8-v%d' % iteration\n",
    "    env = make_environment(name,iteration)\n",
    "    \n",
    "    OBSERVATION_SPACE = env.observation_space.n\n",
    "    ACTION_SPACE = env.action_space.n\n",
    "    \n",
    "    collect = []\n",
    "    zerostates = set()\n",
    "    \n",
    "    q, rpe, collect, zerostates = trainer(iteration,epochs=1000)\n",
    "    collect = array(collect)\n",
    "    policy = make_policy(OBSERVATION_SPACE,q)\n",
    "    model = make_model(collect)\n",
    "    \n",
    "    OBS_SQR = int(math.sqrt(OBSERVATION_SPACE))\n",
    "    STATEGRID = np.zeros((OBS_SQR,OBS_SQR))\n",
    "    \n",
    "    grid = show_policy(STATEGRID)\n",
    "    \n",
    "    D[name] = {}\n",
    "    D = fill_dict(D,name,env,q,zerostates,policy,grid,model)\n",
    "    \n",
    "    env.close()\n",
    "    del name,env,q,zerostates,policy,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('saved/collected.pkl', 'wb')\n",
    "pickle.dump(D, output)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
